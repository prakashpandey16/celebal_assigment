# ğŸš€ Personal Data Migration & File Format Conversion (PySpark + Databricks)

This project performs a complete ETL pipeline using PySpark in Databricks. It involves:
- Loading raw CSV data
- Cleaning column names
- Writing to multiple formats: CSV, Parquet, Avro
- Saving to a MySQL database
- Copying entire tables between personal and portfolio databases

---

## ğŸ› ï¸ Technologies Used
- **Apache Spark (PySpark)**
- **Databricks Community Edition**
- **MySQL (via JDBC)**
- **File Formats:** CSV, Parquet, Avro

---

## âœ… Step-by-Step Code Documentation

### ğŸ“¥ Step 1: Export Data from Database as Raw CSV and Load into DBFS(databricks file system)

## âœ… Step 2: Clean Column Names

**ğŸ” Purpose:**  
Ensure compatibility with file formats and databases by replacing **spaces** (`" "`) and **dots** (`"."`) in column names with **underscores** (`"_"`).


## âœ… Step 3: Write to Multiple File Formats (CSV, Parquet, Avro)

**ğŸ” Purpose:**  
Save the cleaned dataset in multiple formats â€” **CSV**, **Parquet**, and **Avro** â€” to ensure flexibility and compatibility in downstream data processing, analytics, and storage systems.

## âœ… Step 4: Write Cleaned Data to MySQL (Portfolio DB)

**ğŸ” Purpose:**  
Store the cleaned data in a **MySQL portfolio database** to enable **structured querying**, **dashboarding**, or **integration with applications**.

%md
## âœ… Step 5: Copy Multiple Tables from Personal DB to  my Portfolio DB

# ğŸ“ Project Structure

Project Root/
â”œâ”€â”€ Data_pipeline_code   # PySpark and ETL pipeline scripts  
â”œâ”€â”€ sample_data       # Input CSV or mock data files  
â”œâ”€â”€ manifest.mf           # Metadata or config file  
â””â”€â”€ readme                # Project overview or instructions



## ğŸ‘¨â€ğŸ’» Author

**Prakash Pandey** 

- ğŸ”— [GitHub](https://github.com/prakashpandey16)  
- ğŸ”— [LinkedIn](https://www.linkedin.com/in/prakash-pandey-2827522b1/)

