
---

## 📝 Quick Overview

This assignment demonstrates:

- ✅ Secure data movement from local servers using **Self-hosted Integration Runtime (SHIR)**  
- 📂 Integration of **FTP/SFTP data sources** into Azure Data Factory  
- 🔁 Building **incremental data pipelines** with **daily automation**  
- 📆 Configuring **monthly triggers** for the last Saturday of the month  
- 🔄 Implementing **retry logic** for transient failure recovery  

These components together form the foundation of reliable and scalable cloud-based data workflows in modern data engineering.

---

## 📄 Solution Files

| File Name         | Description |
|------------------|-------------|
| [`solution1.md`](solution1.md) | Configure Self-hosted Integration Runtime and load data from a local server into Azure SQL Database. |
| [`solution2.md`](solution2.md) | Set up FTP/SFTP connection and create a pipeline to extract data in ADF. |
| [`solution3.md`](solution3.md) | Design an incremental data load pipeline with watermarking/change tracking and daily automation. |
| [`solution4.md`](solution4.md) | Automate a pipeline to trigger on the last Saturday of every month using custom time-based triggers. |
| [`solution5.md`](solution5.md) | Implement retry logic to handle transient failures during data extraction and processing. |

---

Feel free to explore each file for detailed implementation steps and explanations.

## 👨‍💻 Author

**Prakash Pandey**  
📧 [LinkedIn](nkedin.com/in/prakash-pandey-2827522b1/)  
🛠️ GitHub: [@prakashpandey16](https://github.com/prakashpandey16)

